@INPROCEEDINGS{990517,
  author={P. {Viola} and M. {Jones}},
  booktitle={Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001}, 
  title={Rapid object detection using a boosted cascade of simple features}, 
  year={2001},
  volume={1},
  number={},
  pages={I-I},
  abstract={This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
  keywords={object detection;image classification;image representation;learning (artificial intelligence);feature extraction;rapid object detection;boosted simple feature cascade;machine learning;visual object detection;image processing;image representation;integral image;AdaBoost;classifiers;background regions;object specific focus-of-attention mechanism;statistical guarantees;face detection;real-time applications;Object detection;Face detection;Pixel;Detectors;Filters;Machine learning;Image representation;Focusing;Skin;Robustness},
  doi={10.1109/CVPR.2001.990517},
  ISSN={1063-6919},
  month={Dec},}

@online{cartoonset,
	author = {Google},
	title = {Cartoon Set: An Image Dataset of Random Characters},
	url = {https://google.github.io/cartoonset/index.html},
	urldate = {2020-11-30}
}

@INPROCEEDINGS{7410776,
  author={S. {Yang} and P. {Luo} and C. {Loy} and X. {Tang}},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={From Facial Parts Responses to Face Detection: A Deep Learning Approach}, 
  year={2015},
  volume={},
  number={},
  pages={3676-3684},
  abstract={In this paper, we propose a novel deep convolutional network (DCN) that achieves outstanding performance on FDDB, PASCAL Face, and AFW. Specifically, our method achieves a high recall rate of 90.99% on the challenging FDDB benchmark, outperforming the state-of-the-art method [23] by a large margin of 2.91%. Importantly, we consider finding faces from a new perspective through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variation, which are the main difficulty and bottleneck of most existing face detection approaches. We show that despite the use of DCN, our network can achieve practical runtime speed.},
  keywords={face recognition;learning (artificial intelligence);facial parts responses;face detection;deep learning approach;novel deep convolutional network;DCN;FDDB;PASCAL Face;AFW;scoring facial parts;spatial structure;spatial arrangement;unconstrained pose variation;Face;Face detection;Proposals;Hair;Mouth;Detectors;Nose},
  doi={10.1109/ICCV.2015.419},
  ISSN={2380-7504},
  month={Dec},}

@INPROCEEDINGS{8549735,
  author={K. {Candra Kirana} and S. {Wibawanto} and H. {Wahyu Herwanto}},
  booktitle={2018 International Seminar on Application for Technology of Information and Communication}, 
  title={Facial Emotion Recognition Based on Viola-Jones Algorithm in the Learning Environment}, 
  year={2018},
  volume={},
  number={},
  pages={406-410},
  abstract={An emotion is a trigger of learning success, so the learning should be adapting to the students' emotions. The most of popular approach is the acquisition of facial-based features. Therefore, we present facial emotion recognition based on the Viola-Jones Algorithm in the learning environment. Basically, the Viola-Jones algorithm is a face detection algorithm. However, we use facial-based features to detect face and recognize emotion, thus we applied rectangular feature and cascading AdaBoost algorithm which are the main concept of the Viola-Jones Algorithm in those both of process. In this study, we compare accuracy, precision, recall, and time-consuming of the Viola-Jones algorithm and our previous methods [1] using 50 UM's learning images in student emotion recognition. The accuracy, precision, recall, and time-consuming of Viola-Jones algorithm reach 0.74, 0.73, 0.76 and 15 seconds per frame, whereas our previous methods [1] reach 0.46, 0.48, 0.52, and 42 seconds per frame. In emotional recognition, we can conclude that the viola jones algorithm is superior to our previous research.},
  keywords={emotion recognition;face recognition;feature extraction;learning (artificial intelligence);object detection;rectangular feature;student emotion recognition;viola jones algorithm;facial emotion recognition;learning environment;facial-based features;face detection algorithm;time 42.0 s;time 0.74 s;time 0.73 s;time 0.76 s;time 15.0 s;time 0.46 s;time 0.48 s;time 0.52 s;Emotion recognition;Classification algorithms;Face detection;Face;Seminars;Feature extraction;Biological neural networks;facial recognition;emotion recognition;learning emotion;Viola-Jones algorithm},
  doi={10.1109/ISEMANTIC.2018.8549735},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{8806572,
  author={W. {LU} and M. {YANG}},
  booktitle={2019 International Conference on Robots   Intelligent System (ICRIS)}, 
  title={Face Detection Based on Viola-Jones Algorithm Applying Composite Features}, 
  year={2019},
  volume={},
  number={},
  pages={82-85},
  abstract={Viola-Jones' face detection algorithm was jointly proposed by Paul Viola and Michael Jones. Although it realized face real-time detection to some extent, its false detection rate is not low. Because the block features in the Viola-Jones algorithm can't handle purely rigid objects, such as chopsticks and cups, so if there are rigid objects in the face image, Viola-Jones' face detection algorithm is prone to generate false detection of faces. In this paper, we propose to apply the composite features based on Viola-Jones algorithm to improve the above problems, and prove the feasibility of this method through experiments.},
  keywords={face recognition;object detection;Viola-Jones' face detection algorithm;Paul Viola;Michael Jones;false detection rate;face image;Viola-Jones algorithm;composite features;face real-time detection;Face;Feature extraction;Face recognition;Face detection;Classification algorithms;Compounds;Linear discriminant analysis;Viola-Jones;Compound Features;Face Detection},
  doi={10.1109/ICRIS.2019.00029},
  ISSN={},
  month={June},}

@INPROCEEDINGS{6619290,
  author={Y. {Sun} and X. {Wang} and X. {Tang}},
  booktitle={2013 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Deep Convolutional Network Cascade for Facial Point Detection}, 
  year={2013},
  volume={},
  number={},
  pages={3476-3483},
  abstract={We propose a new approach for estimation of the positions of facial key points with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy key points. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each key point. Second, since the networks are trained to predict all the key points simultaneously, the geometric constraints among key points are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-of-the-art methods in both detection accuracy and reliability.},
  keywords={face recognition;feature extraction;hidden feature removal;image texture;pose estimation;facial keypoint detection accuracy;position estimation;deep convolutional network cascade;three-level convolutional networks;global high-level feature extraction;face region;initialization stage;high accuracy keypoint location;texture context information;geometric constraints;data corruption;image samples;occlusions;pose variations;convolutional network structures;robust facial point detection;reliability;Face;Feature extraction;Convolutional codes;Shape;Training;Detectors;Accuracy;Convolutional Network;Facial Point Detection},
  doi={10.1109/CVPR.2013.446},
  ISSN={1063-6919},
  month={June},}

@ARTICLE{7553523,
  author={K. {Zhang} and Z. {Zhang} and Z. {Li} and Y. {Qiao}},
  journal={IEEE Signal Processing Letters}, 
  title={Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks}, 
  year={2016},
  volume={23},
  number={10},
  pages={1499-1503},
  abstract={Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations, and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this letter, we propose a deep cascaded multitask framework that exploits the inherent correlation between detection and alignment to boost up their performance. In particular, our framework leverages a cascaded architecture with three stages of carefully designed deep convolutional networks to predict face and landmark location in a coarse-to-fine manner. In addition, we propose a new online hard sample mining strategy that further improves the performance in practice. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging face detection dataset and benchmark and WIDER FACE benchmarks for face detection, and annotated facial landmarks in the wild benchmark for face alignment, while keeps real-time performance.},
  keywords={data mining;face recognition;learning (artificial intelligence);annotated facial landmark;detection benchmark;detection dataset;WIDER FACE benchmark;state-of-the-art technique;online hard sample mining strategy;coarse-to-fine manner;landmark location prediction;face location prediction;deep cascaded multitask framework;deep learning approach;unconstrained environment;multitask cascaded convolutional network;joint face detection and alignment;Face;Face detection;Training;Convolution;Detectors;Computer architecture;Benchmark testing;Cascaded convolutional neural network (CNN);face alignment;face detection},
  doi={10.1109/LSP.2016.2603342},
  ISSN={1558-2361},
  month={Oct},}

@INPROCEEDINGS{5170659,
  author={L. {Lu} and Z. {Xu} and P. {Shi}},
  booktitle={2009 WRI World Congress on Computer Science and Information Engineering}, 
  title={Gender Classification of Facial Images Based on Multiple Facial Regions}, 
  year={2009},
  volume={6},
  number={},
  pages={48-52},
  abstract={In this paper, we describe an experimental investigation to evaluate the significance of different facial regions of a person in the task of gender classification. For this purpose we use a support vector machine (SVM) classifier on face images for gender classification. We perform experiments using different facial regions of varying resolution so that the significance of facial regions in this application can be assessed. According to the results obtained, the upper region of the face proved to be the most significant for the task of gender classification. Moreover, the changes in the resolution of the facial region images do not produce significant changes in the result. Based on the significance of different facial regions, we propose a gender classification method based on fusion of multiple facial regions and show that this method is able to compensate for facial expressions and lead to better overall performance.},
  keywords={emotion recognition;face recognition;feature extraction;gender issues;image classification;image fusion;image resolution;support vector machines;gender classification;facial image;support vector machine;SVM;image resolution;image fusion;facial expression;feature extraction;Support vector machines;Support vector machine classification;Face detection;Humans;Face recognition;Computer vision;Principal component analysis;Classification tree analysis;Nose;Computer science;Gender Classification;Support Vector Machine;Multiple Facial Regions},
  doi={10.1109/CSIE.2009.871},
  ISSN={},
  month={March},}

@book{goodfellow_2016,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	note={\url{http://www.deeplearningbook.org}},
	year={2016}
}

@book{bishop_2006,
	place={New York, USA},
	title={Pattern Recognition and Machine Learning},
	publisher={Springer},
	author={Bishop, Christopher M},
	year={2006}
}

@INPROCEEDINGS{6391660,
  author={P. {Goel} and S. {Agarwal}},
  booktitle={2012 International Conference on Computing Sciences}, 
  title={Hybrid Approach of Haar Cascade Classifiers and Geometrical Properties of Facial Features Applied to Illumination Invariant Gender Classification System}, 
  year={2012},
  volume={},
  number={},
  pages={132-136},
  abstract={This paper proposes a fast and efficient approach for gender classification under non uniform illumination variations. Haar Cascade Classifiers are used for face detection from an image. Facial feature extraction from detected face is done by using combined approach of Haar Cascade Classifiers and geometrical properties of facial features. Preliminary facial features viz. eyes, nose and mouth are extracted using Open CV Haar Cascade Classifiers. Further, geometrical properties of facial features are used for eyebrow detection. To further make our approach faster and reduce time complexity, we have used regions which contains moustache and beard. Weber illumination normalization technique is employed to compensate non uniform illumination variations from detected facial features. Support Vector Machine (SVM) is used as classifier for gender classification. Experimental results on Color FERET database and Caltech database show that the proposed approach improves gender classification rate upto 98.75 % along with significantly reduced computing time.},
  keywords={face recognition;feature extraction;gender issues;Haar transforms;image classification;support vector machines;hybrid approach;Haar cascade classifiers;geometrical properties;facial features;illumination invariant gender classification system;face detection;facial feature extraction;open CV Haar cascade classifiers;eyebrow detection;time complexity;support vector machine;SVM;Feature extraction;Face;Facial features;Support vector machine classification;Eyebrows;Lighting;Face Detection;Gender Classification;Haar Cascade Classifiers;Support Vector Machine},
  doi={10.1109/ICCS.2012.40},
  ISSN={},
  month={Sep.},}
  
  

@INPROCEEDINGS{6909637,
  author={V. {Kazemi} and J. {Sullivan}},
  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={One millisecond face alignment with an ensemble of regression trees}, 
  year={2014},
  volume={},
  number={},
  pages={1867-1874},
  abstract={This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efficient feature selection. Different regularization strategies and its importance to combat overfitting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.},
  keywords={face recognition;feature selection;gradient methods;learning (artificial intelligence);optimisation;pose estimation;regression analysis;trees (mathematics);face alignment;regression tree ensemble;face landmark position estimation;pixel intensities;gradient boosting;square error loss sum optimization;feature selection;regularization strategies;overfitting;data augmentation;time 1 ms;Shape;Regression tree analysis;Face;Training;Boosting;Training data;Vectors;Face Alignment;Real-Time;Gradient Boosting;Decision Trees},
  doi={10.1109/CVPR.2014.241},
  ISSN={1063-6919},
  month={June},}

@ONLINE{dlib_shape,
author={Dlib},
title={Dlib Shape Predictor}
url={http://dlib.net/ml.html#shape_predictor_trainer},
urldate = {2020-11-30}
}

@inproceedings{takayama_2012,
	title={Face Detection and Face Recognition of Cartoon Characters using Feature Extraction},
	author={K. {Takayama} and H. {Johan} and T. {Nishita}},
	year={2012}
}

@INPROCEEDINGS{7789587,
  author={K. {Zhang} and L. {Tan} and Z. {Li} and Y. {Qiao}},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={Gender and Smile Classification Using Deep Convolutional Neural Networks}, 
  year={2016},
  volume={},
  number={},
  pages={739-743},
  abstract={Facial gender and smile classification in unconstrained environment is challenging due to the invertible and large variations of face images. In this paper, we propose a deep model composed of GNet and SNet for these two tasks. We leverage the multi-task learning and the general-to-specific fine-tuning scheme to enhance the performance of our model. Our strategies exploit the inherent correlation between face identity, smile, gender and other face attributes to relieve the problem of over-fitting on small training set and improve the classification performance. We also propose the tasks-aware face cropping scheme to extract attribute-specific regions. The experimental results on the ChaLearn 16 FotW dataset for gender and smile classification demonstrate the effectiveness of our proposed methods.},
  keywords={face recognition;image classification;learning (artificial intelligence);neural nets;deep convolutional neural networks;smile classification;unconstrained environment;offace images;GNet;SNet;multitask learning;general-to-specific fine-tuning;face identity;training set;task-aware face cropping;attribute-specific region extraction;facial gender classification;Conferences;Computer vision;Pattern recognition},
  doi={10.1109/CVPRW.2016.97},
  ISSN={2160-7516},
  month={June},}

@article{1907_13394,
	title     = {iCartoonFace: {A} Benchmark of Cartoon Person Recognition},
	journal   = {CoRR},
	volume    = {abs/1907.13394},
	year      = {2019},
	note      = {Withdrawn.},
	url       = {http://arxiv.org/abs/1907.13394},
	archivePrefix = {arXiv},
	eprint    = {1907.13394},
	timestamp = {Tue, 03 Sep 2019 15:35:37 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1907-13394.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{8100196,
  author={W. {Liu} and Y. {Wen} and Z. {Yu} and M. {Li} and B. {Raj} and L. {Song}},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={SphereFace: Deep Hypersphere Embedding for Face Recognition}, 
  year={2017},
  volume={},
  number={},
  pages={6738-6746},
  abstract={This paper addresses deep face recognition (FR) problem under open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. However, few existing algorithms can effectively achieve this criterion. To this end, we propose the angular softmax (A-Softmax) loss that enables convolutional neural networks (CNNs) to learn angularly discriminative features. Geometrically, A-Softmax loss can be viewed as imposing discriminative constraints on a hypersphere manifold, which intrinsically matches the prior that faces also lie on a manifold. Moreover, the size of angular margin can be quantitatively adjusted by a parameter m. We further derive specific m to approximate the ideal feature criterion. Extensive analysis and experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace Challenge 1 show the superiority of A-Softmax loss in FR tasks.},
  keywords={face recognition;feature extraction;image representation;learning (artificial intelligence);neural nets;SphereFace;deep face recognition problem;open-set protocol;ideal face features;smaller maximal intra-class distance;minimal inter-class distance;angular softmax loss;convolutional neural networks;angularly discriminative features;A-Softmax loss;discriminative constraints;hypersphere manifold;angular margin;FR tasks;metric space;ideal feature criterion;labeled face;Youtube faces;Face;Training;Measurement;Face recognition;Manifolds;Testing;Feature extraction},
  doi={10.1109/CVPR.2017.713},
  ISSN={1063-6919},
  month={July},}

@INPROCEEDINGS{8953658,
  author={J. {Deng} and J. {Guo} and N. {Xue} and S. {Zafeiriou}},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={ArcFace: Additive Angular Margin Loss for Deep Face Recognition}, 
  year={2019},
  volume={},
  number={},
  pages={4685-4694},
  abstract={One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that can enhance the discriminative power. Centre loss penalises the distance between deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in the angular space and therefore penalises the angles between deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to its exact correspondence to geodesic distance on a hypersphere. We present arguably the most extensive experimental evaluation against all recent state-of-the-art face recognition methods on ten face recognition benchmarks which includes a new large-scale image database with trillions of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state of the art and can be easily implemented with negligible computational overhead. To facilitate future research, the code has been made available.},
  keywords={convolutional neural nets;face recognition;learning (artificial intelligence);matrix algebra;visual databases;ArcFace;Additive Angular Margin Loss;Deep face recognition;feature learning;Deep Convolutional Neural Networks;large-scale face recognition;appropriate loss functions;discriminative power;centre loss;deep features;Euclidean space;intra-class compactness;linear transformation matrix;fully connected layer;angular space;face class separability;highly discriminative features;exact correspondence;face recognition methods;face recognition benchmarks;class centres;Biometrics;Face;Gesture;and Body Pose ; Recognition: Detection;Categorization;Retrieval},
  doi={10.1109/CVPR.2019.00482},
  ISSN={2575-7075},
  month={June},}

@inproceedings{soukupova2016,
	title={Eye blink detection using facial landmarks},	
	author={Soukupova, Tereza and Cech, Jan},
	booktitle={21st computer vision winter workshop, Rimske Toplice, Slovenia},
	year={2016}
}

@online{ibug_dataset,
	author = {Christos {Sagonas} and Stefanos {Zafeiriou}},
	title = {Facial point annotations},
	url = {https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/},
	urldate = {2020-11-30}
}

@inproceedings{8751886,
	author = {Kir Savaş, Burcu and Becerkli, Yaşar},
	year = {2018},
	month = {10},
	pages = {1-4},
	title = {Real Time Driver Fatigue Detection Based on SVM Algorithm},
	doi = {10.1109/CEIT.2018.8751886}
}